{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit"
  },
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading your own data (only on google colab)\n",
    "# from google.colab import files\n",
    "# path_to_file = list(files.upload().keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Length of the text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read and decode the file\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "print('Length of the text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding\n",
    "vocab = sorted(set(text))   # saves only unique characters because it's set and can't have duplicates\n",
    "\n",
    "char2idx = {u: i for i, u in enumerate(vocab)} # make a dict of words that return a number\n",
    "idx2char = np.array(vocab)                     # make a numpy array that returns a work with given integer\n",
    "\n",
    "def text_to_int(text):\n",
    "    return np.array([char2idx[c] for c in text])\n",
    "\n",
    "text_as_int = text_to_int(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "print(len(text_as_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Text: First Citizen\nEncoded [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "print(\"Text:\", text[:13])\n",
    "print(\"Encoded\", text_to_int(text[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First Citizen\n"
     ]
    }
   ],
   "source": [
    "# convert numberic value to text\n",
    "def int_to_text(ints):\n",
    "    try:\n",
    "        ints = ints.numpy()\n",
    "    except:\n",
    "        pass\n",
    "    return ''.join(idx2char[ints])\n",
    "\n",
    "print(int_to_text(text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training examples\n",
    "seq_length = 100        # length of sequence for a training example\n",
    "examples_per_epochs = len(text)//(seq_length+1)\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into input and output\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]     # hell\n",
    "    target_text = chunk[1:]     # ello\n",
    "    return input_text, target_text  # hell, ello\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\nEXAMPLE\n\nINPUT\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou\n\nOUTPUT\nirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou \n\n\nEXAMPLE\n\nINPUT\nare all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you \n\nOUTPUT\nre all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you k\n"
     ]
    }
   ],
   "source": [
    "for x, y in dataset.take(2):\n",
    "    print(\"\\n\\nEXAMPLE\\n\")\n",
    "    print(\"INPUT\")\n",
    "    print(int_to_text(x))\n",
    "    print(\"\\nOUTPUT\")\n",
    "    print(int_to_text(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make training batches\n",
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(vocab)     # number of unique characters\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_6\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_6 (Embedding)      (64, None, 256)           16640     \n_________________________________________________________________\nlstm_6 (LSTM)                (64, None, 1024)          5246976   \n_________________________________________________________________\ndense_6 (Dense)              (64, None, 65)            66625     \n=================================================================\nTotal params: 5,330,241\nTrainable params: 5,330,241\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building the model\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),     # 64 entries and unknown sequence length\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)   # all chars get a probability of being written\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in data.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)      # ask untrained model for a prediction on ouly the first batch of training data\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "64\ntf.Tensor(\n[[[ 7.77695968e-04  6.39738224e-04  2.03169463e-03 ... -1.89285493e-05\n   -1.54321711e-03 -6.05596974e-03]\n  [-8.45483039e-04  1.07012864e-03 -3.36983101e-03 ... -6.74847933e-03\n   -2.01403117e-03 -2.45625083e-03]\n  [-1.26469228e-03  1.54576194e-03 -1.04580971e-03 ... -1.81564060e-03\n   -2.36053276e-03  1.66473945e-03]\n  ...\n  [-3.41779739e-03 -7.40789808e-03 -5.19507006e-03 ... -1.76967867e-03\n    8.51488207e-03 -3.68018099e-03]\n  [-1.38771138e-03 -4.85506980e-03 -2.54219258e-03 ...  3.75598553e-04\n    4.44052042e-03 -1.01981899e-02]\n  [ 3.45741352e-03  8.05082615e-04  2.27783108e-04 ...  1.22378115e-03\n   -2.73963762e-03 -6.89533213e-03]]\n\n [[-3.96347605e-05  6.14016084e-03  1.82525895e-03 ...  1.69300579e-03\n    3.04102851e-03  4.15699463e-03]\n  [ 1.69914938e-03  3.37603735e-03  2.83990381e-03 ...  3.43322032e-03\n   -5.48522454e-04  3.64905503e-03]\n  [ 1.09238643e-03  2.78782658e-03 -4.46950505e-03 ... -5.73534332e-03\n   -2.26590689e-03  6.20287471e-03]\n  ...\n  [-4.06057108e-03 -5.14965504e-03 -5.27420081e-03 ...  9.61126853e-03\n    1.09182787e-03 -3.13133863e-03]\n  [-3.12918238e-03 -3.56297195e-03 -3.20440112e-03 ...  1.01510007e-02\n   -4.00399324e-04  1.39821577e-03]\n  [-1.26467436e-03 -8.39327229e-04 -1.05308695e-02 ...  1.02136321e-02\n    3.19627579e-04  1.08639314e-03]]\n\n [[-5.43807074e-03  5.15148102e-04  5.95902558e-04 ... -8.67655035e-05\n    7.29234423e-03 -2.05181935e-03]\n  [-2.98269209e-03 -1.97047973e-03 -1.94886932e-04 ... -3.81821068e-03\n    9.31298174e-03 -2.47327075e-03]\n  [-3.19917919e-04 -3.84057406e-04  2.26561259e-03 ... -1.81897916e-03\n    5.66767808e-03 -8.65767337e-03]\n  ...\n  [ 3.37187597e-03 -5.04693016e-05 -7.25302612e-03 ...  6.54311525e-03\n   -4.19298559e-03  1.28125423e-03]\n  [ 2.59722164e-03 -2.60083936e-04 -1.07318293e-02 ... -3.20398761e-03\n   -4.50497540e-03  3.11182672e-03]\n  [ 3.90154822e-03 -9.78723168e-04 -5.94895286e-03 ... -3.41617409e-03\n   -6.14734553e-03 -4.30835551e-03]]\n\n ...\n\n [[-9.09940340e-04  7.28291401e-04  5.90930274e-03 ... -3.01594310e-03\n   -3.26124765e-03  3.95983225e-04]\n  [ 1.96696725e-03  3.36239906e-03  6.44378830e-03 ...  2.69403402e-03\n   -3.18935188e-03  6.19129161e-04]\n  [ 3.27540794e-03  1.62334647e-03  2.12810910e-03 ... -3.17013077e-03\n   -2.88995588e-03 -7.25585269e-04]\n  ...\n  [ 9.67154652e-03 -5.59187494e-04 -7.87567534e-03 ... -8.57869163e-04\n    1.62344612e-03 -4.67386143e-03]\n  [ 1.13235377e-02  2.50594760e-03 -8.17579404e-03 ... -2.60394346e-03\n   -3.41884606e-03 -6.13088207e-03]\n  [ 1.54439164e-02  4.33882605e-03 -9.90573596e-03 ... -4.68543265e-04\n    5.77624422e-04 -6.61224267e-03]]\n\n [[ 7.77695968e-04  6.39738224e-04  2.03169463e-03 ... -1.89285493e-05\n   -1.54321711e-03 -6.05596974e-03]\n  [-1.92938233e-03 -5.10370173e-03  2.63245124e-03 ... -2.03146040e-03\n    1.39210955e-03 -2.97955656e-03]\n  [-6.21468062e-03 -4.31118021e-03  4.21495503e-03 ... -4.09154221e-04\n    6.56587258e-03 -2.98063504e-03]\n  ...\n  [-4.36659902e-03  3.11241625e-03 -4.58846521e-03 ... -5.36157656e-03\n    7.91958161e-03 -4.83418442e-03]\n  [-2.21682596e-03  4.02662670e-03 -1.17424484e-02 ... -3.82347149e-04\n    6.37428323e-03 -4.72014584e-03]\n  [-3.85535625e-03 -4.44801524e-04 -8.27377848e-03 ... -2.89988704e-03\n    4.42128815e-03 -5.26988041e-03]]\n\n [[ 2.42218259e-03  2.79790908e-03 -2.00743764e-03 ... -2.27182917e-03\n   -4.81588393e-03 -1.18243194e-03]\n  [ 2.93872645e-03  3.75367654e-03  8.58117768e-04 ... -2.05940194e-03\n   -4.79372870e-03 -6.58065779e-03]\n  [-1.66047830e-05 -2.08556326e-03  1.83204212e-03 ... -3.83007387e-03\n   -7.81430863e-04 -3.09742405e-03]\n  ...\n  [ 6.55319169e-03 -6.08019414e-04  1.97626115e-03 ...  8.09821999e-04\n   -3.73922242e-03 -6.66566705e-03]\n  [ 8.89382884e-03 -6.87886728e-04  1.52290403e-03 ...  1.24652521e-03\n   -9.04929265e-03 -5.41023351e-03]\n  [ 6.17813086e-03 -6.67351112e-03  9.88344080e-04 ... -2.05735490e-03\n   -4.36407421e-03 -2.91761360e-03]]], shape=(64, 100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# prediction is an array of 64 arrays (a tensor), one for each entry in the batch\n",
    "print(len(example_batch_predictions))\n",
    "print(example_batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "100\ntf.Tensor(\n[[ 7.7769597e-04  6.3973822e-04  2.0316946e-03 ... -1.8928549e-05\n  -1.5432171e-03 -6.0559697e-03]\n [-8.4548304e-04  1.0701286e-03 -3.3698310e-03 ... -6.7484793e-03\n  -2.0140312e-03 -2.4562508e-03]\n [-1.2646923e-03  1.5457619e-03 -1.0458097e-03 ... -1.8156406e-03\n  -2.3605328e-03  1.6647395e-03]\n ...\n [-3.4177974e-03 -7.4078981e-03 -5.1950701e-03 ... -1.7696787e-03\n   8.5148821e-03 -3.6801810e-03]\n [-1.3877114e-03 -4.8550698e-03 -2.5421926e-03 ...  3.7559855e-04\n   4.4405204e-03 -1.0198190e-02]\n [ 3.4574135e-03  8.0508261e-04  2.2778311e-04 ...  1.2237811e-03\n  -2.7396376e-03 -6.8953321e-03]], shape=(100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# lets examine one prediction\n",
    "pred = example_batch_predictions[0]\n",
    "print(len(pred))\n",
    "print(pred)\n",
    "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "65\ntf.Tensor(\n[ 7.7769597e-04  6.3973822e-04  2.0316946e-03 -1.1612432e-02\n -3.7100972e-03  2.8371441e-03 -4.1937972e-03  2.2567075e-04\n -4.8389155e-03 -5.4234769e-03  2.1775011e-03 -5.0175041e-03\n -2.8521391e-03 -2.6248413e-04  2.6501841e-03 -2.0666046e-03\n  2.7472714e-03  2.3458211e-03 -4.8187417e-03 -8.7728025e-05\n  3.5929581e-05 -2.8522322e-03 -7.3567466e-03  2.0985084e-04\n  2.1653438e-03  6.5829963e-03  1.2334986e-03 -2.7452749e-03\n -1.3528244e-03 -6.4258225e-04 -2.6518479e-03 -3.5489060e-03\n -1.1602850e-03 -8.2205544e-04 -8.7511633e-04  1.4177201e-03\n  1.6514213e-03 -6.8266492e-04 -1.4768521e-03  2.7670744e-03\n  2.3668727e-03 -3.0195098e-03 -5.3482191e-03  3.2652649e-03\n -4.9783257e-03 -2.3275407e-03 -4.6913689e-03 -5.6165177e-03\n  8.1272540e-04 -1.0322529e-03  2.8428598e-03  9.7590196e-04\n -2.5865491e-03  5.4621941e-04 -2.1409472e-03 -1.9982690e-04\n  5.3626411e-03 -1.9531995e-03  2.2694974e-03 -2.7325761e-04\n -5.0106095e-03 -9.4954460e-04 -1.8928549e-05 -1.5432171e-03\n -6.0559697e-03], shape=(65,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# finally we look at a prediction at the first timestep\n",
    "time_pred = pred[0]\n",
    "print(len(time_pred))\n",
    "print(time_pred)\n",
    "# and of course its 65 values representing the probabillity of each character occuring next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"zmGrM&szBYScZ.sLi.f-AGzOYE,qSRwF-PNwyp3DuhVyY.;Ibt:vvA,?eHDpaUHDOhqUPQGM.ouvxt?I\\nmJ3KzO'A,Pu,,FVzjFT\""
      ]
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "# Determine the predicted character\n",
    "sampled_indices = tf.random.categorical(pred, num_samples=1) # sampling the probability instead of picking max value from prediction (GOOGLE IT!!)\n",
    "\n",
    "# reshape array and convert all the integers to numbers to see the actual characters\n",
    "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
    "predicted_chars = int_to_text(sampled_indices)\n",
    "\n",
    "predicted_chars     # what model predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS FUNCTION\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating checkpoints in dir\n",
    "checkpoint_dir = \"./training_checkpoints\"\n",
    "# Name of checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "172/172 [==============================] - 31s 166ms/step - loss: 2.6678\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "history = model.fit(data, epochs=10, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latest checkpoint\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load specific checkpoint\n",
    "# checkpoint_num = 10\n",
    "# model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n",
    "# model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating text\n",
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 800\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "    \n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "romeon, apance on os lit\nThes houls'sen, I ary thou no'ce base\nhom hentoless,\ne buteagf'd. Is for Whatpey,\nMowid. hull's flevebpeteas if so mer.\nAnd I te mart sy lath opcinput.\nI\nLPRSHARA:\nBy yow sle gove.\n\nGesGrof:\nTe dige thass boncelll be nis?\nFret young.\n\nFiETt:\nSo bwallo Gaverecith;\nAnd of wath limid y ue to thourrer,\nCrovaread sroul wor and bucond'sw andt. I andy\nTpronty me deem:\nUrdor linf Esather theneds, wort rof ay;\nher of sreainst dad e ceakidn me,\n\nVARDLiS:\nMir i, intherseath, we pard froua's gravedt\nCathin sace wedeed he youthurs\nMather: my hackith ghegtlowss\nOPlved:\nI amen't:\nYey warld efarothirs fuesser:\nAnd hamt, is apearbde.\nWhan low': alid; FUny yot lith vee hlveld brcithet:\nBunseer meele.\nWhyous\nOnd an mo crouncy'sed bred,\nWheill'bbsand lids, bode, of spatyt,\nHour, wingequry \n"
     ]
    }
   ],
   "source": [
    "inp = input(\"Type a starting string: \")\n",
    "print(generate_text(model, inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}